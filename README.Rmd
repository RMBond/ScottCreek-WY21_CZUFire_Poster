---
title: "Scott Creek CZU Fire 2021 SRF Poster Notes"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  github_document:
    toc: true
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


## Introduction
This is a space to keep track of the datasets used in the 2021 SRF poster. The goal is to visualize the "first flush" after the 2020 CZU Fire in the Scott Creek watershed (Santa Cruz, CA). Initially we plan to look at water quality and pebble count data.

<br>

**Dataset Descriptions** 


The <span style="color:purple">*Data*</span> folder contains the datasets used in the poster.

1. The <span style="color:purple">*Scott_Creek_Weir_Hydrolab_20210303.csv*</span> datafile contains a snippbet (incomplete dataset) of the water quality data collected by the HYDROLAB (S/N 66279, model DS5X). The unit is installed directly above the weir. It collects depth, temp, salinity, turbidity, (and other) every half hour. This file be updated as new downloads are completed up untill the time of the poster. Note there is some column renaming done before importing the dataset into r.

2. The <span style="color:purple">*Gagedata_XXX.csv*</span> datafile contains a portion of the stage(timestamp and ft) data. The unit is installed near the Archibald Creek consluence with the mainstem. While we won't be converting stage to discharge (we need a new rating curve), it will be helpful for visualizing the hyrograph and relate this to the WQ data. <span style="color:red">Unfortunately this file is missing WY20 data (specifically data between 12/4/19 and 12/4/20)and it's likely we will not be presenting this data. We can use the pressure data in the WQ dataset as a proxy.</span>

3. The <span style="color:purple">*Scott_Creek_Pebble_20210319.csv*</span> datafile consists of pebble counts at 23 transects accrost the watershed (6 eFishing sites with 3 transects each, and 5 additional pebble count only transects) which were repeated twice (Autumn 2020 & Winter 2021). AC and MA entered and QC'ed the data. 


## Next Steps

1. Decide on which WQ parameters to present and continue to update WQ data csv file as needed. 

2. RB is working on code for making pebble count summaries and potential plotting.

```{r, Package Setup, echo = FALSE, message = FALSE}
library(tidyverse)
library(dplyr)
library(lubridate)
library(ggplot2)
library(patchwork)

```



## WQ Data

Goal: Visualize WQ time-series during the "first flush". 

```{r WQ Data Wrangling, echo = FALSE}

wq.dat <- read.csv("Data/Scott_Creek_Weir_Hydrolab_20210303.csv", sep = ",", header = T) #Initial water qualiy test dataset.

wq <- wq.dat %>% #Start formatting columns into r "tidy" structure.
  mutate(Date = mdy(Date)) %>% #R sees dates as Year-Month-Day this tells it where to get that info from the date column.
  mutate(Time = hms(Time)) %>% # Tells r this column is Hours:Min:Sec
  mutate(TS = as.POSIXct(Date + Time)) %>% #making a new column with timestamp by combining Date and Time
  select(Date, TS, Temp_C, TurbSC_NTU, Dep100_m, pH, Sal_ppt, LDO_mg_l)#pair down dataset to what we need

```
Dataset name = wq

Variable (column) description:

* Date = date 
* TS = Timestamp
* Temp_C = Water Temperature [*C]
* TurbSC_NTU = Turbidity [NTU]
* Dep100_m =  Water depth (think height of water column) [m]
* pH = pH
* Sal_ppt = Salinity [parts per thousand] 
* LDO_mg_l = Dissolved Oxygen [mg/L]

```{r}
str(wq)
```

```{r, WQ Plotting, echo = FALSE, message = FALSE, warning = FALSE}

#Set data limits
limits = as.POSIXct(c("2021-01-01 00:00:00", "2021-03-01 00:00:00"))

#Start making plots
#Water depth
depth.plot <- ggplot(wq, aes(x = TS, y = Dep100_m)) +
  geom_line() +
  scale_x_datetime(name = "",
                   date_breaks = "1 week", date_labels = ("%b %d"),
                   limits = limits, expand = c(0,0)) +
  scale_y_continuous(name = "Height [m]", limits = c(1.5, 3.5)) +
theme_classic()

#Turbidity
turb.plot <- ggplot(wq, aes(x = TS, y = TurbSC_NTU)) +
  geom_line() +
  scale_x_datetime(name = "",
                   date_breaks = "1 week", date_labels = ("%b %d"),
                   limits = limits, expand = c(0,0)) +
  scale_y_continuous(name = "Turbidity [NTU]", limits = c(0, 3000)) +
theme_classic()

#pH
pH.plot <- ggplot(wq, aes(x = TS, y = pH)) +
  geom_line() +
  scale_x_datetime(name = "",
                   date_breaks = "1 week", date_labels = ("%b %d"),
                   limits = limits, expand = c(0,0)) +
  scale_y_continuous(name = "pH", limits = c(7, 8)) +
theme_classic()

#DO
do.plot <- ggplot(wq, aes(x = TS, y = LDO_mg_l)) +
  geom_line() +
  scale_x_datetime(name = "",
                   date_breaks = "1 week", date_labels = ("%b %d"),
                   limits = limits, expand = c(0,0)) +
  scale_y_continuous(name = "DO [mg/L]", limits = c(8.5, 12.1)) +
theme_classic()

#Temperature
temp.plot <- ggplot(wq, aes(x = TS, y = Temp_C)) +
  geom_line() +
  scale_x_datetime(name = "",
                   date_breaks = "1 week", date_labels = ("%b %d"),
                   limits = limits, expand = c(0,0)) +
  scale_y_continuous(name = "Temperature [*C]", limits = c(6.5, 25.5)) +
theme_classic()


# #Salinity - not much to see since its freshwater the whole time.
# sal.plot <- ggplot(wq, aes(x = TS, y = Sal_ppt)) +
#   geom_line() +
#   scale_x_datetime(name = "",
#                    date_breaks = "1 week", date_labels = ("%b %d"),
#                    limits = limits, expand = c(0,0)) +
#   scale_y_continuous(name = "Salinity [ppt]", limits = c(8.5, 12.1)) +
# theme_classic()


#Using Patchwork pckage to put individual plots together into one multi-plot
depth.plot / turb.plot / pH.plot / do.plot / temp.plot

```


## Hydrograph

Goal: Visualize Hydrograph time-series during the "first flush". Note this data will *not* be presented in the poster.

```{r Hydrograph Data Wrangling}
# library(lubridate)
# library(ggplot2)
# library(scales)
# library(gridExtra)
# library(dplyr)
# options(scipen = 999)

# #Bring in datasets
# gage.dat <- read.csv("data/Gagedata_XXX.csv", h = T)#Flows up to 30 January 2021 6891 obs of 2 var.
# 
# #Clean up hydrograph data
# gage <- gage.dat %>% #Start formatting columns into r "tidy" structure.
#   mutate(DT = mdy_hm(as.character(timestamp))) %>% # clean up time data
#   mutate(stage_m = stage_ft*0.3048) %>% #Convert from ft to m
#   select(DT, stage_m)
# 
# str(gage)
# 
# #Hydrograph 
# #Plot 1 - basic plot
# ggplot(gage,aes(DT,stage_m)) +
#   geom_point() +
#   scale_x_datetime(name = "",
#                    date_breaks = "1 month", date_labels = ("%b")) +
#   scale_y_continuous(name = "Height (m)") +
# theme_classic() 
```


## Pebble Counts 

Goals: 

1. Compare how different surface summary statistics (i.e. Dx) changed after the “first flush” at 1.A each site and 1.B longitudinally along the mainstem and 

2. Estimate the change in the amount of surface fines at each transect (reported as % change over time). Note: The cuttoff for "fines" in the litterature is a bit ambiguous. For now we will stick with "fines" meaning <6mm (though <8mm could also been used).

* There are some example papers vizualizing pebble count data in the CZU fire google drive [pebble count folder](https://drive.google.com/drive/u/1/folders/1MwYFVTyhN1_3NMqhlwIAu8DBYOm8KzSo).


```{r Pebble Count Data Wrangling, echo = FALSE}

#Test Data
pc.dat.test <- read.csv("Data/Scott_Creek_Pebble_Testdata_20210315.csv", sep = ",", header = T) #Initial pebble count test dataset. It consists of one transects worth of data.

pc.test <- pc.dat.test %>% #Start formatting columns into r "tidy" structure.
  mutate(Date = mdy(Date)) %>% 
  mutate(Round = as.factor(Round)) %>% #change column to type = factor
  mutate(Long_Station = as.numeric(Long_Station)) %>% #change column to type = numeric.
  mutate(Category_total = as.numeric(Category_total)) %>% #change column to type = numeric.
  arrange(Size_class_mm) %>% #makes sure data goes from smallest to larges pebble sizes.
  mutate(Percent_finer = cumsum(Category_total)/sum(Category_total)) #creat column for percent finer than.



#Real (whole) Data
pc.dat <- read.csv("Data/Scott_Creek_Pebble_20210319.csv", sep = ",", header = T) #736 obs of 7 var.

pc <- pc.dat %>% #Start formatting columns into r "tidy" structure.
  mutate(Date = ymd(Date)) %>% 
  mutate(Round = as.factor(Round)) %>% #change column to type = factor
  mutate(Long_Station = as.numeric(Long_Station)) %>% #change column to type = numeric.
  mutate(Category_total = as.numeric(Category_total)) %>% #change column to type = numeric.
  mutate(Transect = replace_na(Transect, 0)) %>% #replace na (for PCX transects) with a zero 
  group_by(Round, Site, Transect) %>% #Tells r to group each individual transect worth of data to calculate % finer.
  arrange(Size_class_mm) %>% #makes sure data goes from smallest to larges pebble sizes.
  mutate(Percent_finer = cumsum(Category_total)/sum(Category_total)) #creat column for percent finer than. 


```

Dataset name = pc

Variable (column) description:

* Date = Sample date (format = YYYY-MM-DD).

* Site = Site name (“___ eFishing” or “PCX-__”).

* Transect = Transect number (1,2,3 for eFishing sites or blank for PCX sites).

* Round = Survey round (1 = Autumn 2020, 2 = Winter 2021).

* Long_Station = Longitude Station (1 = Lower eFishing, 2 = PCX-1, 3 = PCX-2, 4 = PCX-4, 5 = PCX-5, 6 = Upper eFishing, 7 = Dog eFishing, 8 = PCX-3, 9 = Big Creek eFishing, 10 = Powerhouse eFishing, 11 = Little Creek eFishing).

* Size_class_mm = Size class (mm), gevelometer hole the pebble *did not* fit though.

* Category_total = Total number of pebbles in the size class.

* Percent_finer = Cumulative percent finer for each size class. This is used for calculating the Dx statistic. 

```{r}
str(pc)
```


```{r}

#A quick plot of one transect

#Make a test dataset using Big Creek Transect 3 with color noting each round.
test.bc <- pc %>% 
  filter(Site == "Big Creek eFishing")

#Plot
ggplot(test.bc, aes(Size_class_mm, Percent_finer, color = Round)) +
  geom_line() +
  facet_grid(Transect ~ .) +
  scale_x_log10(name = "Partical size [mm]") +
  scale_y_continuous(name = "Cumulative percent finer", limits = c(0,1), expand = c(0,.1)) +
  theme_classic()
```



#Find gransize at percentiles

Colin Nicol has generously shared his code to help with this. He created a function which interpolates a straight line between the two points nearest to the desired percentile `Dx`. Using the data provided, the function looks for the minimum grain size where the percent finer is greater than `Dx`. Then it calculates the slope between those two lines. From here, it uses the slope and the `rise` to get to 50% to calculate a `run` (distance on the x-axis `grain size` we need to move from the known point to `D50`).   

```{r Pebble Count Summary Stats (Dx)}

#Pebble Count Summary Stat code from C. Nicol :)

calculate_dx <- function(dx, size, prcnt_finer) {
  # Purpose: Calculate the grainsize for a given percentile
  # Method: Interpolate a straight line between the two bounding points
  # Input:  df - Dataframe with percent finer than data, dx - desired output percentile
  
  # Output: Grainsize at dx
  
  dx <- dx/100
  
  data.frame(size = size, prcnt_finer = prcnt_finer) %>%
    mutate(abovex = min(size[prcnt_finer > dx]),
           lessx = max(size[prcnt_finer <= dx])) %>%
    filter(size %in% c(abovex, lessx)) %>%
    mutate(slope = (max(prcnt_finer) - min(prcnt_finer)) / (max(size) - min(size))) %>%
    filter(size == lessx) %>%
    mutate(run = (dx - prcnt_finer) / slope,
           dx_size = size + run,
           dx = dx * 100) %>%
    pull(dx_size)
  
}

#Call in what you want to calculate. In this case its the median grain size (D50) of the test dataset (one site). 

calculate_dx(50, pc.test$Size_class_mm, pc.test$Percent_finer)

#Note, it will give you an error if the first category (i.e. <2mm) is greater than the percential you want to calculate.

# calculate_dx(16, pc.test$Size_class_mm, pc.test$Percent_finer) #error generated becuase you want to calculate the D16 and <2mm is 40% of the sample.

#Once we have this function set up, loop through D16, D50 and D84 for the one site.
# dxs <- c(16, 50, 84) # choose which percentiles to calculate (e.g. D16, D50, D84)
# 
# names(dxs) <- paste0('d', dxs)
# 
# sapply(dxs, calculate_dx, size = pc.test$Size_class_mm, prcnt_finer = pc.test$Percent_finer) 



# Calculate summary stats for multiple transects all at once
#NOT WORKING 
# pc2 <- pc %>%
#   group_by(Site, Transect, Round) %>% # Creates a unique grouping variable
#   summarize(
#     d50 = calculate_dx(50, size = pc$Size_class_mm, prcnt_finer = pc$Percent_finer),
#     d84 = calculate_dx(84, size = pc$Size_class_mm, prcnt_finer = pc$Percent_finer))

```

# Caculate change in % fines (<6mm) for each transect 
Start by summarising the % fines at each transect and then subtract round 1 from round 2. Poitive numbers mean the channel bed has gotten finer and negative numbers mean the bed has gotten more course.

```{r Caculate change in % fines (<6mm) for each transect}

#Subset data by Round
pc3.r1 <- pc %>% 
  filter(Round == 1) %>% #Round 1 data
  filter(Size_class_mm == 5.6) #focus on the 5.6 size class for each transect


pc3.r2 <- pc %>% 
filter(Round == 2) %>% #Round 2 data
  filter(Size_class_mm == 5.6) #focus on the 5.6 size class for each transect

#Spread data into long format to calculate % change
pc3 <- full_join(pc3.r1, pc3.r2, by = c("Site", "Transect")) %>% #Joins the two rounds of data
  mutate(fines_per_change = Percent_finer.y - Percent_finer.x) %>% #Subract round 1 from round 2. A positive number means an increase in fines after the first flush.
  mutate(Long_Station = Long_Station.x) %>% #relabeling station for clarity
  select(Site, Transect, Long_Station, fines_per_change)

#Plot % fines change over longitudinal station distance

#Subset data by mainstem vs tributary stations (See above for station number key)
pc3.mainstem <- pc3 %>% 
  filter(Long_Station < 8) %>% 
  filter(Transect == 0 | Transect == 2)

pc3.bigcreek <- pc3 %>% 
  filter(Long_Station > 7 & Long_Station < 11) %>% 
  filter(Transect == 0 | Transect == 2)

pc3.littlecreek <- pc3 %>% 
  filter(Long_Station == 11) %>% 
  filter(Transect == 0 | Transect == 2)

#Subset efishing reaches to make ranges for plot
stattest <- pc3 %>% 
  filter(Long_Station == 7)

#Mainstem plot

ggplot(pc3.mainstem, aes(x = Long_Station, y = fines_per_change)) +
  geom_line() +
  scale_y_continuous(name = "Percent Change (<6mm)", limits = c(-0.2,0.45), breaks = seq(-0.2,0.45,.1), expand = c(0,0)) +
  scale_x_continuous(name = "Longitudinal Distance", limits = c(1,7), breaks = seq(0,7,1)) +
  theme_classic() +
  geom_hline(yintercept = 0, lty = 2) +
  geom_segment(x = 1, y = -0.13, xend = 1, yend = 0.08, lty = 3) + #variation in Station 1
  geom_segment(x = 6, y = 0.12, xend = 6, yend = 0.25, lty = 3) + #variation in Station 6
  geom_segment(x = 7, y = -0.11, xend = 7, yend = 0.3857, lty = 3) + #variation in Station 7
  labs(title = "Change in percent fines (<6mm) along the mainstem", 
       subtitle = "Flow is from right (Upstream) to left (Downstream)",
       caption = "The meainstem between Big and Little Creeks (Station 3) and the upper watershed \n (Stations 6 and 7) had the biggest increases in fine sediment. \n Dotted vertical lines are ranges at the eFishing sites.")
  


```


## Figure Output 


The <span style="color:purple">*Figure*</span> folder contains the figures used in the poster.

1.